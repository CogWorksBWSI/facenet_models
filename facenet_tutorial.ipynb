{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facenet Tutorial\n",
    "\n",
    "Hey guys! Today we're going to use facnet_pytorch to help you with some handy-dandy face recognition. Facenet-pytorch is a [PyTorch](https://pytorch.org/) implementation of [this paper](https://arxiv.org/abs/1503.03832) *FaceNet: A unified Embedding for Face Recognition and Clustering*, which achieves state-of-the-art accuracy in face recognition.\n",
    "\n",
    "**Why are we using facenet-pytorch?** You might be thinking, why not just work on another neural net to learn to detect and recognize faces? Two things: data, and time. In order to achieve a model with accuracy as good as facenet-pytorch, we'd need tons of labeled data that's pretty hard to come by. And, to create and train a model of that magnitude would take much more time than we have avaliable. However, you all already know the basic concepts of what's going on under the hood, and it's more convienent and accurate to use this model.\n",
    "\n",
    "First, you're going to want to clone and do the whole `python setup.py develop` thing to use the facenet models - you can find the repo [here](https://github.com/CogWorksBWSI/facenet_models).\n",
    "\n",
    "Make sure you've already run through the Camera Tutorial module to set up your camera and to check if the ports are correct, etc! (you can find Camera [here](https://github.com/LLCogWorks2018/Camera) if you haven't set it up yet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to setup matplotlib, and also import the very important take_picture function from camera!\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from camera import take_picture\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a picture to test this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "pic = take_picture()\n",
    "ax.imshow(pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello gorgeous!! Now, let's see if we can detect any faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_models import FacenetModel\n",
    "\n",
    "model = FacenetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, probabilities, landmarks = model.detect(pic)\n",
    "\n",
    "print(f\"Number of faces detected: {len(boxes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heck yeah, we got a face, which is a step in the right direction. The facenet-pytorch model gives us a set of boxes containing the face regions of our image and a set of landmarks, which correspond to specific facial attributes: your eyes, corners of your mouth, and nose. Let's visualize those features now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(pic)\n",
    "\n",
    "\n",
    "for box, prob, landmark in zip(boxes, probabilities, landmarks):\n",
    "    # draw the box on the screen\n",
    "    ax.add_patch(Rectangle(box[:2], *(box[2:] - box[:2]), fill=None, lw=2, color=\"red\"))\n",
    "\n",
    "    # Get the landmarks/parts for the face in box d.\n",
    "    # Draw the face landmarks on the screen.\n",
    "    for i in range(len(landmark)):\n",
    "        ax.plot(landmark[i, 0], landmark[i, 1], '+', color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're able to find faces in an image! However, in order to match names with faces, we need to be able to distinguish *between* faces. Luckily, our model has this cool feature called a \"face descriptor vector\". Think of a face descriptor as a numerical representation of the intricacies of your face. We'll tell the model where the faces are by passing in the boxes and it will give us the face descriptors, which are 512-dimensional vectors encoding a person's identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look as to what the descriptor is!!\n",
    "descriptor = model.compute_descriptors(pic, boxes)\n",
    "print(descriptor.shape)\n",
    "print(descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah, that's pretty cool!! As you can see, the **descriptor vector** looks like a pretty standard NumPy array. Each descriptor vector is different. Even if we take descriptor vectors of the same face in two different pictures, they'll be slightly different. Now, the problem lies in how we can distinguish between the minute details in faces of the same people and the large differences in other faces. \n",
    "\n",
    "There's a pretty tried and true formula for finding this out, and it should look a little familiar. We can calculate the distance between each point in the vector, add it all up, and see if it's greater or less than a certain threshold value that we can determine ourselves.\n",
    "\n",
    "differences = $\\sqrt{\\sum{((p1 - p2)^2)}}$\n",
    "\n",
    "As a note, we square each difference between points to make sure it isn't negative and messing up our total. Then, we square root it all at the end to even it back out.\n",
    "\n",
    "Alright, now it's your turn! Grab a friend and take a picture. As a note, you'll get two detections, two shapes, and two descriptor values, so remember to account for them both. Try seeing how different your faces are. Then, maybe take two pictures of yourself and see the difference between the two. Try to come up with a good threshold value between determining different people. \n",
    "\n",
    "Have fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
